# @package _global_
# メインHydra設定ファイル
# 他の設定ファイルを組み合わせてトレーニング設定を構築

defaults:
  - model: default
  - data: default
  - trainer: default
  - optimizer: default
  - scheduler: default
  - callbacks: default
  - logger: wandb
  - experiment: null  # 実験設定で上書き可能
  - _self_

# グローバル設定
seed: 42
project_name: "diffusion-lm-pretraining"

# 出力ディレクトリ（Hydraが自動生成）
output_dir: ${hydra:runtime.cwd}/workdir/${now:%Y-%m-%d}/${now:%H-%M-%S}

# トレーニングスケジュール計算用
# これらの値はtrain.pyで動的に計算されます
training:
  flops_budget: 10.0e18  # デフォルト: 10e18 FLOPs
  model_params: null  # モデル設定から注入
  global_batch_size: null  # データ設定から注入
  block_size: 2048
  # 以下の値はPythonコードで計算されます（eval:は使えないため）
  max_steps: null  # 計算式: flops_budget / (6 * model_params * global_batch_size * block_size)
  warmup_steps: null  # 計算式: max(max_steps // 100, 100)
  gradient_accumulation_steps: null  # 計算式: global_batch_size // (devices * micro_batch_size)

# Hydra実行時設定
hydra:
  run:
    dir: ${output_dir}
  job:
    chdir: true  # 実行ディレクトリを出力ディレクトリに変更
  sweep:
    dir: ${hydra:runtime.cwd}/workdir/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.num}
