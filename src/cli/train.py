"""
ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Hydra + PyTorch Lightning)

ä½¿ç”¨ä¾‹:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
    python -m src.cli.train

    # ç‰¹å®šã®å®Ÿé¨“è¨­å®šã‚’ä½¿ç”¨
    python -m src.cli.train experiment=scaling_170M

    # è¨­å®šã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
    python -m src.cli.train experiment=scaling_170M data.batch_size=512 trainer.devices=8

    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰
    python -m src.cli.train experiment=debug

    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ—
    python -m src.cli.train -m optimizer.lr=1e-4,2e-4,5e-4 data.batch_size=128,256
"""

import os
from pathlib import Path

import hydra
import lightning as L
from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint
from lightning.pytorch.loggers import CSVLogger, WandbLogger
from omegaconf import DictConfig, OmegaConf


@hydra.main(version_base=None, config_path="../../configs", config_name="config")
def main(cfg: DictConfig) -> None:
    """
    ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–¢æ•°

    HydraãŒè¨­å®šã‚’è‡ªå‹•çš„ã«èª­ã¿è¾¼ã¿ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã‚’é©ç”¨ã—ã¾ã™ã€‚

    Args:
        cfg: Hydraã«ã‚ˆã£ã¦èª­ã¿è¾¼ã¾ã‚ŒãŸè¨­å®š
    """

    # è¨­å®šã‚’è¡¨ç¤ºï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
    # print("=" * 80)
    # print("Training Configuration:")
    # print("=" * 80)
    # print(OmegaConf.to_yaml(cfg))
    # print("=" * 80)

    # ã‚·ãƒ¼ãƒ‰è¨­å®š
    L.seed_everything(cfg.seed, workers=True)

    # DataModuleåˆæœŸåŒ–
    print("\n[1/4] Initializing DataModule...")
    datamodule = hydra.utils.instantiate(cfg.data)
    print(f"  âœ“ Data directory: {datamodule.data_dir}")
    print(f"  âœ“ Global batch size: {datamodule.global_batch_size}")
    print(f"  âœ“ Micro batch size: {datamodule.micro_batch_size}")

    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¨ˆç®—
    # eval:ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯OmegaConfã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œãªã„ãŸã‚ã€Pythonã§è¨ˆç®—
    print("\nCalculating training schedule...")
    model_params = cfg.model.num_parameters
    global_batch_size = datamodule.global_batch_size
    block_size = cfg.training.block_size
    flops_budget = cfg.training.flops_budget

    # max_steps = flops_budget / (6 * model_params * tokens_per_step)
    tokens_per_step = global_batch_size * block_size
    max_steps = int(flops_budget / (6 * model_params * tokens_per_step))
    warmup_steps = max(max_steps // 100, 100)

    # gradient_accumulation_steps = global_batch_size // (devices * micro_batch_size)
    devices = cfg.trainer.devices if isinstance(cfg.trainer.devices, int) else len(cfg.trainer.devices)
    gradient_accumulation_steps = global_batch_size // (devices * datamodule.micro_batch_size)

    print(f"  âœ“ Max steps: {max_steps}")
    print(f"  âœ“ Warmup steps: {warmup_steps}")
    print(f"  âœ“ Gradient accumulation steps: {gradient_accumulation_steps}")

    # è¨­å®šã‚’æ›´æ–°ï¼ˆå¾Œã§å‚ç…§ã™ã‚‹ãŸã‚ï¼‰
    OmegaConf.update(cfg, "training.model_params", model_params)
    OmegaConf.update(cfg, "training.global_batch_size", global_batch_size)
    OmegaConf.update(cfg, "training.max_steps", max_steps)
    OmegaConf.update(cfg, "training.warmup_steps", warmup_steps)
    OmegaConf.update(cfg, "training.gradient_accumulation_steps", gradient_accumulation_steps)

    # ModelåˆæœŸåŒ–
    print("\n[2/4] Initializing Model...")
    # optimizer_config ã¨ scheduler_config ã‚’ Model ã«æ¸¡ã™
    model_kwargs = {
        "model_config": cfg.model.model_config,
        "optimizer_config": OmegaConf.to_container(cfg.optimizer, resolve=True),
        "scheduler_config": OmegaConf.to_container(cfg.scheduler, resolve=True),
        "num_parameters": cfg.model.num_parameters,
    }
    model = hydra.utils.instantiate(cfg.model, **model_kwargs, _recursive_=False)
    print(f"  âœ“ Model: {cfg.model.model_config.name}")
    print(f"  âœ“ Parameters: {cfg.model.num_parameters / 1e6:.2f}M")
    print(f"  âœ“ Layers: {cfg.model.model_config.n_layer}")
    print(f"  âœ“ Hidden size: {cfg.model.model_config.n_embd}")

    # CallbacksåˆæœŸåŒ–
    print("\n[3/4] Initializing Callbacks...")
    callbacks = []
    if "callbacks" in cfg and cfg.callbacks is not None:
        for callback_name, callback_cfg in cfg.callbacks.items():
            if callback_cfg is not None:
                cb = hydra.utils.instantiate(callback_cfg)
                callbacks.append(cb)
                print(f"  âœ“ {callback_name}: {type(cb).__name__}")

    # LoggeråˆæœŸåŒ–
    print("\n[4/4] Initializing Logger...")
    logger = None
    if "logger" in cfg and cfg.logger is not None:
        logger = hydra.utils.instantiate(cfg.logger)
        print(f"  âœ“ Logger: {type(logger).__name__}")
        if isinstance(logger, WandbLogger):
            print(f"  âœ“ Project: {logger._project}")
            print(f"  âœ“ Name: {logger._name}")
    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§CSVLoggerã‚’ä½¿ç”¨
        logger = CSVLogger(save_dir=cfg.output_dir, name="training_logs")
        print(f"  âœ“ Logger: CSVLogger")

    # TraineråˆæœŸåŒ–
    print("\n" + "=" * 80)
    print("Initializing Trainer...")
    print("=" * 80)

    trainer_kwargs = OmegaConf.to_container(cfg.trainer, resolve=True)

    trainer = L.Trainer(
        **trainer_kwargs,
        callbacks=callbacks,
        logger=logger,
        default_root_dir=cfg.output_dir,
    )

    print(f"  âœ“ Accelerator: {trainer.accelerator}")
    print(f"  âœ“ Devices: {cfg.trainer.devices}")
    print(f"  âœ“ Precision: {cfg.trainer.precision}")
    print(f"  âœ“ Max steps: {cfg.trainer.max_steps}")
    print(f"  âœ“ Gradient accumulation: {cfg.training.gradient_accumulation_steps}")

    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æƒ…å ±ã®è¡¨ç¤º
    print("\n" + "=" * 80)
    print("Training Schedule:")
    print("=" * 80)
    print(f"  FLOPs budget: {cfg.training.flops_budget / 1e18:.2f}e18")
    print(f"  Max steps: {cfg.training.max_steps}")
    print(f"  Warmup steps: {cfg.training.warmup_steps}")
    print(f"  Global batch size: {cfg.training.global_batch_size}")
    print(f"  Tokens per step: {cfg.training.global_batch_size * cfg.training.block_size}")
    print("=" * 80)

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å†é–‹
    ckpt_path = None
    if "resume_from" in cfg and cfg.resume_from is not None:
        ckpt_path = cfg.resume_from
        print(f"\nâš  Resuming from checkpoint: {ckpt_path}")

    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹
    print("\n" + "=" * 80)
    print("ğŸš€ Starting Training...")
    print("=" * 80)

    try:
        trainer.fit(model, datamodule=datamodule, ckpt_path=ckpt_path)
    except KeyboardInterrupt:
        print("\nâš  Training interrupted by user")
    except Exception as e:
        print(f"\nâŒ Training failed with error: {e}")
        raise

    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†
    print("\n" + "=" * 80)
    print("âœ… Training completed!")
    print("=" * 80)

    # çµæœã‚µãƒãƒªãƒ¼
    if hasattr(trainer, "callback_metrics"):
        print("\nFinal metrics:")
        for key, value in trainer.callback_metrics.items():
            print(f"  {key}: {value}")

    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ã®è¡¨ç¤º
    if callbacks:
        for cb in callbacks:
            if isinstance(cb, ModelCheckpoint):
                print(f"\nğŸ’¾ Best checkpoint: {cb.best_model_path}")
                print(f"ğŸ’¾ Last checkpoint: {cb.last_model_path}")


if __name__ == "__main__":
    main()
